from refdata_hdf5_class import refdata
from dE_MLP import *

import matplotlib.pyplot as plt
import matplotlib.lines as lines
import matplotlib.animation as animation
import matplotlib as mpl
from matplotlib import rc
from mpl_toolkits.mplot3d import axes3d
import numpy as np
import pickle
import jax
import jax.numpy as jnp
import jax.random as random

font = {'size' : 10}
mpl.rc('font', **font)
#mpl.rcParams["text.usetex"] = True
mpl.rcParams['font.family'] = "sans-serif"
#mpl.rcParams['font.sans-serif'] = "Helvetica" # still not right
mpl.rcParams['xtick.labelsize'] = 8
mpl.rcParams['ytick.labelsize'] = 8
#mpl.rcParams['axes.labelweight'] = "bold"
mpl.rcParams['legend.fontsize'] = 8
mpl.rcParams['lines.linewidth'] = 0.8

class NNPlot():
    """Class to (at somepoint ...) generate all graphs used in the BT relating to NN analysis"""
    def __init__(self, name = "test", layers = None, params_file = None , loss_file = None, axes = None):
        """
        @params name:str        --> name of instance, used to generate file names
        @params layers:list     --> list representation of the layer geometry
        @params params_file:str --> pkl file containing the params 
        @params loss_file:str   --> pkl file containing the train and val loss
        @params axes:int        --> number of axes present in the final graph (None or 2 currently working)
        """
        if params_file != None:
            self.params:list = self.load_params(params_file) #fname = params
            self.params_f = self.params[-1]
        if loss_file != None:
            self.train_loss:list = self.load_loss(loss_file)[0]
            self.val_loss:list = self.load_loss(loss_file)[1]
        if layers != None:
            self.layers:list = layers
        elif params_file != None:
            self.layers:list = self.check_layer_architecture()

        self.name = name
        self.fig = plt.figure(self.name)
        self.axes = axes
        return
    
    def load_loss(self, fname):
        """returns a loss value list of [train, val]"""
        with open(fname, "rb") as f:
            loss = pickle.load(f)
        loss = list(np.array(loss).T)
        return loss
    
    def load_params(self, fname = None):
        """loads the params_hist_X file generated by MLP.py and returns a list of numpy array """
        with open(fname, "rb") as f:
            params = pickle.load(f)
        flat = []
        out = []
        for par in params:
            lay = []
            for layer in par:
                lay.append([np.array(layer[0]), np.array(layer[1])])
            out.append(lay)
        self.params = out
        return out
    
    def check_layer_architecture(self):
        """Checks self.params_f and generates the layer architecture"""
        layer_architecture = []
        for i, layer in enumerate(self.params_f):
            w,b = layer
            layer_architecture.append(w.shape[1])
            if i == len(self.params_f)-1:
                layer_architecture.append(w.shape[0])

        return layer_architecture
    
    def load_samples(self, n_samples, fname = "trainingsdata/3000K_methanol.hdf5", convert_to_rics=True, permu = False):
        """
        Loads data from hdf5, currently only energy implemented
        @params n_samples:int       --> number of samples load from training and val respecitvly
        @params fname:str           --> path to hdf5 file
        return                      --> train_energies, train_rics, val_energies, val_rics
        """
        r = refdata(fname = fname)
        all_xyz, all_id = r.get_xyz_from_sample(sample = "MD")
        _max = len(all_xyz)
        if n_samples != None:
            M = n_samples
        else:
            M = _max
        r.change_lot("TPSS")
        train_DFT_forces = r.get_forces_from_id([x for x in range(M)])
        dft0_id = r.get_lowest_energy()# get lowest energy INDEX in dataset
        dft0_E = r.get_energies_from_id(dft0_id)
        train_DFT_energies = r.get_energies_from_id([x for x in range(M)])
        train_DFT_energies -= dft0_E # Set refenergy
        r.change_lot("MOFFF")
        train_FF_energies = r.get_energies_from_id([x for x in range(M)])
        train_FF_forces = r.get_forces_from_id([x for x in range(M)])
        ff0_E = r.get_energies_from_id(dft0_id)
        train_FF_energies -= ff0_E
        train_energies = train_DFT_energies - train_FF_energies # Delta learning
        train_forces = train_DFT_forces - train_FF_forces
        train_xyz = all_xyz[:M]
        # validation set
        r.change_lot("TPSS")
        val_DFT_energies = r.get_energies_from_id([x for x in range(M,_max)]) # methods only produces "forward" indexing 
        val_DFT_forces = r.get_forces_from_id([x for x in range(M,_max)])
        val_DFT_energies -= dft0_E
        r.change_lot("MOFFF")
        val_FF_energies = r.get_energies_from_id([x for x in range(M,_max)])
        val_FF_forces = r.get_forces_from_id([x for x in range(M,_max)])
        val_forces = val_DFT_forces - val_FF_forces
        val_FF_energies -= ff0_E
        val_energies = val_DFT_energies - val_FF_energies # Delta learning
        val_xyz = all_xyz[M:_max]
        r.close()
        if permu == True:
            val_xyz[:,[1,2]] = val_xyz[:,[2,1]]
            val_xyz[:,[2,3]] = val_xyz[:,[3,2]]
            train_xyz[:,[1,2]] = train_xyz[:,[2,1]]
            train_xyz[:,[2,3]] = train_xyz[:,[3,2]]

            #val_xyz[:,[1,5]] = val_xyz[:,[5,1]] #sanity check with non permuatable switch
            #train_xyz[:,[1,5]] = train_xyz[:,[5,1]]            
        if convert_to_rics == True:
            __m = molsys.mol.from_file("methanol.mfpx")
            bondtable = jnp.array(__m.get_conn_as_tab())
            conn_table = __m.conn
            angletable = gen_angle_table(conn_table)
            dihedraltable = gen_dihedral_table(conn_table)
            ric_tables = (bondtable, angletable, dihedraltable)
            train_rics = mbatch_RICs(train_xyz, ric_tables)
            val_rics = mbatch_RICs(val_xyz, ric_tables)
            return train_energies, train_rics, val_energies, val_rics, train_forces, val_forces
        else:
            return train_energies, train_xyz, val_energies, val_xyz, train_forces, val_forces

    def MLP_error_barplot(self, inp, ref, _id = None, error = False, area=None):
        """
        Creates a bar plot with the E(DFT)-E(FF) on y and each bar representing one structure
        in purple the reference values and in darkcyan the mlp prediction which is generated on the fly
        @params inp:array        --> input vectors for MLP (for example: RICs)
        @params ref:array        --> reference value vector for input, sorted in the same way as inp
        return              --> ax
        """
        from dE_MLP import mbatch_MLP
        from dE_MLP import mbatch_loss
        if _id == None:
            params = self.params_f
        else:
            params = self.params[_id]
        sort = ref.argsort()
        sort_inp = inp[sort]
        sort_ref = ref.flatten()[sort]# + 0.495 

        mlp_value = mbatch_MLP(params, sort_inp)#-0.495
        _id = np.arange(len(mlp_value))
        delta = mlp_value - sort_ref
        ax = self.fig.add_subplot(111)
        print("loss value:",mbatch_loss(params, sort_inp, sort_ref))
        if error == False:
            ref_bar = ax.bar(_id, sort_ref, width=1, alpha=0.75, label = "ref", color = "purple") # energy difference of DFT and FF from ref file
            mlp_bar = ax.bar(_id, mlp_value, width=1, alpha=0.75, label = "mlp", color = "darkcyan") # energy differnece prediction of MLP
            ax.set_ylabel(r"$\Delta E$")
        elif error == True: #ABS error
            _error = delta
            if area == "up":
                filter = np.where(np.abs(_error)>0.04)
                e_id = np.array(_id[filter], dtype='<U5')
                e_error = _error[filter]
                print(e_error)
                print(e_id)
                error_bar = ax.bar(e_id, e_error, width=1, alpha=0.75, label = "abs. error", color = "purple")
                ax.set_ylabel(r"$\Delta \Delta E$")
                for tick in ax.xaxis.get_major_ticks()[1::2]:
                    tick.set_pad(15)
            if area == "low":
                filter = np.where(np.abs(_error)<0.00005)
                e_id = np.array(_id[filter], dtype='<U5')
                e_error = _error[filter]
                print(e_error)
                print(e_id)
                error_bar = ax.bar(e_id, e_error, width=1, alpha=0.75, label = "abs. error", color = "purple")
                ax.set_ylabel(r"$\Delta \Delta E$")
                for tick in ax.xaxis.get_major_ticks()[1::2]:
                    tick.set_pad(15)

            _error = delta/sort_ref *100
            error_bar = ax.bar(_id[700:900], _error[700:900], width=1, alpha=0.75, label = "rel. error", color = "purple")
            ax.set_ylabel(r"$\frac{\Delta \Delta E}{ref} [\%]$")
        ax.legend()
        ax.set_xlabel("structure")
        
        return ax

    def MLP_error_barplot_force(self, inp, ref, _id = None):
        """
        Creates a bar plot with the E(DFT)-E(FF) on y and each bar representing one structure
        in purple the reference values and in darkcyan the mlp prediction which is generated on the fly
        @params inp:array        --> input vectors TO BE CONVERTED -> xyz 
        @params ref:array        --> reference value vector for input, sorted in the same way as inp
        return              --> ax
        """
        from Multi_Layer_Perceptron import mbatch_ini_MLP, grad_mbatch_RICs, mbatch_RICs, mbatch_force_loss
        __m = molsys.mol.from_file("methanol.mfpx")
        bondtable = jnp.array(__m.get_conn_as_tab())
        conn_table = __m.conn
        angletable = gen_angle_table(conn_table)
        dihedraltable = gen_dihedral_table(conn_table)
        ric_tables = (bondtable, angletable, dihedraltable)
        inp_rics = mbatch_RICs(inp, ric_tables)
        dric_dxyz = grad_mbatch_RICs(inp, ric_tables)
        if _id == None:
            params = self.params_f
        else:
            params = self.params[_id]
        
        _, mlp_forces = mbatch_ini_MLP(inp_rics, params, dric_dxyz)

        sort = ref.flatten().argsort()
        sort_ref = ref.flatten()[sort]
        mlp_value = mlp_forces.flatten()[sort]
        print(mbatch_force_loss(inp_rics, params, dric_dxyz, ref))
        
        _id = np.arange(len(mlp_value))
        #delta = mlp_value - sort_ref
        #_error = (delta*100)/sort_ref
        ax = self.fig.add_subplot(111)
        ref_bar = ax.bar(_id, sort_ref, width=1, alpha=0.75, label = "ref", color = "purple") # energy difference of DFT and FF from ref file
        mlp_bar = ax.bar(_id, mlp_value, width=1, alpha=0.75, label = "mlp", color = "darkcyan") # energy differnece prediction of MLP
        ax.legend()
        ax.set_xlabel("Force id")
        ax.set_ylabel(r"$\Delta F$")

        
        return ax
    
    def loss_overview(self, stamname, low = None, up = None):
        """
        generates a plot with the data from all 10 seeds for the train/val loss values
        @param stamname:str         --> base name up until the seed number
        @param low:int              --> lower boundary of displayed data
        @param up:int               --> upper boundary of displayed data
        """
        fig, ax = plt.subplots(5,2, figsize=(8.27,11.69))
        for i in range(10):
            fname = stamname + f"{i}.pkl"
            loss = self.load_loss(fname=fname)
            train = loss[0][low:up]
            val = loss[1][low:up]
            epoch = [x for x in range(len(loss[0]))][low:up]
            ax[i//2, i%2].plot(epoch, train, label="Training", color = "darkcyan", linewidth = 1)
            ax[i//2, i%2].plot(epoch, val, label="Validation", color = "purple", linewidth = 1)
            if i == 8:
                ax[i//2,i%2].set_xlabel("epoch")
                ax[i//2,i%2].set_ylabel("RMS loss")
            if i == 1:
                ax[i//2,i%2].legend()

        return fig
    
    def loss_convergence(self, stamname, identifier = None,low = None, up = None, i=0):
        ax = self.fig.add_subplot(111)
        for _id in identifier:
            fname = stamname + f"{_id}_{i}.pkl"
            loss = self.load_loss(fname=fname)
            val = loss[1][low:up]
            epoch = [x for x in range(len(loss[0]))][low:up]
            ax.plot(epoch, val, label=_id)
        ax.legend()
        ax.set_xlabel("epoch")
        ax.set_ylabel("RMS loss")
        return ax

    def flatten_params(self, params):
        """takes [epoch:layer:(w,b)]"""
        out = []
        for epoch in params:
            for layer in epoch:
                for par in layer:
                    for ele in par.flatten():
                        out.append(ele)
        return out
    
    def NN_loss(self, low = None, up = None):
        """plots val and train loss over epochs"""
        if self.axes == 2:
            ax = self.fig.add_subplot(122)
        else:
            ax = self.fig.add_subplot(111) 
        x = np.arange(len(self.train_loss))[low:up]
        y = self.train_loss[low:up]
        ax.plot(x,y, color = "darkcyan", label = "training")
        y = self.val_loss[low:up]
        ax.plot(x,y, color = "purple", label = "validation")
        ax.set_xlabel("epoch")
        ax.set_ylabel("loss")
        ax.legend()
        return ax

    def NN_weights_bias(self, epoch = -1):
        """show layers of NN with all weights as connections and bias inside neurons"""
        #self.fig = plt.figure()
        if self.axes == 2:
            ax = self.fig.add_subplot(121)
        else:
            ax = self.fig.add_subplot(111) # <-- make dependent on how many plots there will be? --> use plt.subplots?
        ax.set_aspect("equal")
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.spines['bottom'].set_visible(False)
        ax.spines['left'].set_visible(False)
        ax.get_xaxis().set_ticks([])
        ax.get_yaxis().set_ticks([])

        rad = 100/(2*(2*max(self.layers)-1)) # determine the radius of the neurons
        x_max1 = (max(self.layers)*rad*4)/(len(self.layers))*(len(self.layers))
        x_max2 = 8*rad*(len(self.layers)-1)
        if x_max1 > x_max2:
            x_max = x_max1
        else:
            x_max = x_max2
        ax.set_xlim(-(rad*2), x_max+(rad*2))
        ax.set_ylim(-50-(rad*1.5), 50+(rad*1.5))

        par = self.params[epoch]
        par_flat = self.flatten_params(self.params)
        ref_par = np.max(np.abs(np.array(par_flat))) # highest values of bias AND weights of ALL epochs as relativ point for coloring etc


        node_coords = [] # node_coords of all neurons, list of lists --> [[layer0], [layer1], ...]
        for i, layer in enumerate(self.layers):
            pos = []
            if i > 0: # inp layer has "no" weights and bias
                w,b = par[i-1]
            if x_max1 > x_max2: # choose node distance on x axis based on chosed scaling
                x = (max(self.layers)*rad*4*i)/(len(self.layers)-1)
            else:
                x = (8*rad*i)
            if layer%2 == 0:
                for n in range(int(layer/2)):
                    p1 = +(2+4*n)*rad
                    p2 = -(2+4*n)*rad
                    pos.append((x,p1))
                    pos.append((x,p2))
            else:
                for n in range(int(layer/2)+1):
                    if n == 0:
                        p1 = 0
                        pos.append((x,p1))
                    else:
                        p1 = +4*n*rad
                        p2 = -4*n*rad
                        pos.append((x,p1))
                        pos.append((x,p2))
            
            for j in range(len(pos)):
                y = pos[j][1]
                if i > 0:
                    if b[j] >= 0:
                        color = "darkcyan"
                    else:
                        color = "purple"
                    alpha = abs(b[j]/ref_par)
                else:
                    color = "Black" # number them instead?
                    alpha = None
                ax.add_patch(plt.Circle((x,y), radius = rad, ec = mpl.colors.to_rgba("black", 1), linewidth = 1, fc = mpl.colors.to_rgba(color,alpha)))
            node_coords.append(pos)
        
        for nlay in range(len(node_coords)):
            if nlay+1 < len(node_coords):
                _w,_b = par[nlay]
                _w = _w.T       
                for i, pos0 in enumerate(node_coords[nlay]):
                    x0, y0 = pos0
                    for n, pos1 in enumerate(node_coords[nlay+1]):
                        x1, y1 = pos1
                        width = _w[i][n]/ref_par 
                        if _w[i][n] >= 0:
                            col = "darkcyan"
                        else:
                            col = "purple"
                        ax.add_artist(lines.Line2D((x0,x1),(y0,y1), linewidth=width, color = col)) # --> make linewidth dependent on |rel. size| of the weight
        return ax
    
    def animate_NN_weights_bias(self, iter = None):
        """shows all params as animated plot"""
        if self.axes == 2:
            ax = self.fig.add_subplot(121)
        else:
            ax = self.fig.add_subplot(111) # <-- make dependent on how many plots there will be? --> use plt.subplots?
        
        def update(epoch):
            ax.clear()
            ax.set_aspect("equal")
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)
            ax.spines['bottom'].set_visible(False)
            ax.spines['left'].set_visible(False)
            ax.get_xaxis().set_ticks([])
            ax.get_yaxis().set_ticks([])
            ax.set_title("epoch:") # is not updated using "blit = True" in the FuncAnimate function


            rad = 100/(2*(2*max(self.layers)-1)) # determine the radius of the neurons
            x_max1 = (max(self.layers)*rad*4)/(len(self.layers))*(len(self.layers))
            x_max2 = 8*rad*(len(self.layers)-1)
            if x_max1 > x_max2:
                x_max = x_max1
            else:
                x_max = x_max2
            ax.set_xlim(-(rad*2), x_max+(rad*2))
            ax.set_ylim(-50-(rad*1.5), 50+(rad*1.5))

            
            par_flat = self.flatten_params(self.params)
            ref_par = np.max(np.abs(np.array(par_flat))) # highest values of bias AND weights of ALL epochs as relativ point for coloring etc

            
            art = [ax.text(0.95, 0.95, f"{epoch}", horizontalalignment='center',
                   verticalalignment='center', transform=ax.transAxes)]
            par = self.params[epoch]
            node_coords = [] # node_coords of all neurons, list of lists --> [[layer0], [layer1], ...]
            for i, layer in enumerate(self.layers):
                pos = []
                if i > 0: # inp layer has "no" weights and bias
                    w,b = par[i-1]
                if x_max1 > x_max2: # choose node distance on x axis based on chosed scaling
                    x = (max(self.layers)*rad*4*i)/(len(self.layers)-1)
                else:
                    x = (8*rad*i)
                if layer%2 == 0:
                    for n in range(int(layer/2)):
                        p1 = +(2+4*n)*rad
                        p2 = -(2+4*n)*rad
                        pos.append((x,p1))
                        pos.append((x,p2))
                else:
                    for n in range(int(layer/2)+1):
                        if n == 0:
                            p1 = 0
                            pos.append((x,p1))
                        else:
                            p1 = +4*n*rad
                            p2 = -4*n*rad
                            pos.append((x,p1))
                            pos.append((x,p2))
                
                for j in range(len(pos)):
                    y = pos[j][1]
                    if i > 0:
                        if b[j] >= 0:
                            color = "darkcyan"
                        else:
                            color = "purple"
                        alpha = abs(b[j]/ref_par)
                    else:
                        color = "Black" # number them instead?
                        alpha = None
                    tmp = ax.add_patch(plt.Circle((x,y), radius = rad, ec = mpl.colors.to_rgba("black", 1), linewidth = 1, fc = mpl.colors.to_rgba(color,alpha)))
                    art.append(tmp)
                node_coords.append(pos)
            
            for nlay in range(len(node_coords)):
                if nlay+1 < len(node_coords):
                    _w,_b = par[nlay]
                    _w = _w.T       
                    for i, pos0 in enumerate(node_coords[nlay]):
                        x0, y0 = pos0
                        for n, pos1 in enumerate(node_coords[nlay+1]):
                            x1, y1 = pos1
                            width = _w[i][n]/ref_par 
                            if _w[i][n] >= 0:
                                col = "darkcyan"
                            else:
                                col = "purple"
                            tmp = ax.add_artist(lines.Line2D((x0,x1),(y0,y1), linewidth=width, color = col)) # --> make linewidth dependent on |rel. size| of the weight
                            art.append(tmp)
            return art
        
        if iter == None:
            frames = len(self.params)
        else:
            frames = iter
        ani = animation.FuncAnimation(self.fig, update, frames = frames, interval = 1000, blit = True)
        return ani
    
    def plot_anafunc(self):
        self.fig.subplots_adjust(wspace=0.6, hspace=0.6)
        ax1 = self.fig.add_subplot(1,2,1,projection="3d")
        ax1.tick_params(axis='both', which='major', labelsize=5)
        ax1.tick_params(axis='both', which='major', pad=0.1)
        x = np.linspace(-4,4,1000)
        X,Y = np.meshgrid(x,x)
        def f(x,y):
            return (1 - x/2 + x**5 + y**3) * np.exp(-x**2 -y**2)
        Z = f(X,Y)
        ax1.set_xlabel("x", fontsize = 5)
        ax1.set_ylabel("y", fontsize = 5)
        ax1.set_zlabel("z", fontsize = 5)
        ax1.plot_surface(X,Y,Z, cmap=plt.cm.cool)
        ax2 = self.fig.add_subplot(1,2,2)
        ax2.tick_params(axis='both', which='major', labelsize=5)
        #ax2.tick_params(axis='both', which='major', pad=0.1)
        ax2.set_aspect("equal")
        ax2.contourf(X,Y,Z, 9 , alpha = 1, cmap=plt.cm.cool)
        C = ax2.contour(X, Y, Z, 9, colors='black', linewidths=.5)
        ax2.clabel(C, inline=1, fontsize=4)
        ax2.set_xlabel("x", fontsize = 5)
        ax2.set_ylabel("y", fontsize = 5)
        return 

    def plot_anaMLP(self, fname = None, _id = None):
        from MLP_ana_regression import mbatch_MLP, inp_vectors
        print("Make sure the mbatch_MLP uses the right activation function")
        self.fig.subplots_adjust(wspace=0.6, hspace=0.6)
        ax1 = self.fig.add_subplot(1,2,1,projection="3d")
        ax1.tick_params(axis='both', which='major', labelsize=5)
        ax1.tick_params(axis='both', which='major', pad=0.1)
        reso = 1000
        inp = inp_vectors(reso)
        if fname == None:
            params = self.params_f
        else:
            params = self.load_params(fname=fname)[_id]
        print("start")
        Z = np.array(mbatch_MLP(params, inp))
        x = np.linspace(-4,4,reso)
        X,Y = np.meshgrid(x,x)
        Z = Z.reshape(reso,reso)
        ax1.set_xlabel("x", fontsize = 5)
        ax1.set_ylabel("y", fontsize = 5)
        ax1.set_zlabel("z", fontsize = 5)
        ax1.plot_surface(X,Y,Z, cmap=plt.cm.cool)
        ax2 = self.fig.add_subplot(1,2,2)
        ax2.tick_params(axis='both', which='major', labelsize=5)
        #ax2.tick_params(axis='both', which='major', pad=0.1)
        ax2.set_aspect("equal")
        ax2.contourf(X,Y,Z, 9 , alpha = 1, cmap=plt.cm.cool)
        C = ax2.contour(X, Y, Z, 9, colors='black', linewidths=.5)
        ax2.clabel(C, inline=1, fontsize=4)
        ax2.set_xlabel("x", fontsize = 5)
        ax2.set_ylabel("y", fontsize = 5)
        return
    
    def DeltaMLPfunc(self, fname = None, _id = None):
        from MLP_ana_regression import mbatch_MLP, inp_vectors
        print("Make sure the mbatch_MLP uses the right activation function")
        self.fig.subplots_adjust(wspace=0.6, hspace=0.6)
        ax1 = self.fig.add_subplot(1,2,1,projection="3d")
        ax1.tick_params(axis='both', which='major', labelsize=5)
        ax1.tick_params(axis='both', which='major', pad=0.1)
        reso = 1000
        inp = inp_vectors(reso)
        if fname == None and _id == None:
            params = self.params_f
        elif fname == None and _id != None:
            params = self.params[_id]
        else:
            params = self.load_params(fname=fname)[_id]
        print("start")
        Z1 = np.array(mbatch_MLP(params, inp))
        x = np.linspace(-10,10,reso)
        X,Y = np.meshgrid(x,x)
        def f(x,y):
            return (1 - x/2 + x**5 + y**3) * np.exp(-x**2 -y**2)
        Z0 = f(X,Y)
        Z1 = Z1.reshape(reso,reso)
        Z = (Z0-Z1)*1000
        ax1.set_xlabel("x", fontsize = 5)
        ax1.set_ylabel("y", fontsize = 5)
        ax1.set_zlabel("z", fontsize = 5)
        ax1.plot_surface(X,Y,Z, cmap=plt.cm.cool)
        ax2 = self.fig.add_subplot(1,2,2)
        ax2.tick_params(axis='both', which='major', labelsize=5)
        #ax2.tick_params(axis='both', which='major', pad=0.1)
        ax2.set_aspect("equal")
        ax2.contourf(X,Y,Z, 9 , alpha = 1, cmap=plt.cm.cool)
        C = ax2.contour(X, Y, Z, 9, colors='black', linewidths=.5)
        ax2.clabel(C, inline=1, fontsize=5)
        ax2.set_xlabel("x", fontsize = 5)
        ax2.set_ylabel("y", fontsize = 5)
        return

    def inp_histo(self, inp = None, _id = None, xlabel = None, **kwargs):
        """
        creates histrogram of input data w.r.t. a specific element (e.g first element of ric -> C-H1)
        @params inp:array       --> input vectors for MLP
        @params _id:int         --> element index for the inp vector
        """
        fig = plt.figure()
        ax = fig.add_subplot(111)
        ax.set_ylabel("occurence")
        ax.set_xlabel(xlabel)

        element = np.array(inp.T[_id])
        _ = ax.hist(element, 100, density = False, alpha=0.75, facecolor="darkcyan")
        return ax

    def actifunc_plot(self, func, low = -3, up = 3):
        """ plots a given activation function
        @param func     --> a python function taking an x input and return y"""
        x = np.linspace(low, up, 1000)
        y = func(x)

        ax = self.fig.add_subplot(111)
        ax.plot(x,y, color = "purple")
        return ax


    def save_fig(self, dpi = 200, nameadd = ""):
        if nameadd == "":
            plt.savefig(fname = f"{self.name}.png", dpi = dpi, format = "png", bbox_inches = "tight", transparent = False) # pdf, svg does not save the negativ lines ...?!
        else:
            plt.savefig(fname = f"{self.name}_{nameadd}.png", dpi = dpi, format = "png", bbox_inches = "tight", transparent = False)

def main(n_samples = 2000, fname = "trainingsdata/methanol.hdf5", convert_to_rics=False):
    r = refdata(fname = fname)
    all_xyz, all_id = r.get_xyz_from_sample(sample = "MD")
    _max = len(all_xyz)
    if n_samples != None:
        M = n_samples
    else:
        M = _max
    r.change_lot("TPSS")
    train_DFT_forces = r.get_forces_from_id([x for x in range(M)])
    dft0_id = r.get_lowest_energy()# get lowest energy INDEX in dataset
    dft0_E = r.get_energies_from_id(dft0_id)
    train_DFT_energies = r.get_energies_from_id([x for x in range(M)])
    train_DFT_energies -= dft0_E # Set refenergy
    r.change_lot("MOFFF")
    train_FF_energies = r.get_energies_from_id([x for x in range(M)])
    train_FF_forces = r.get_forces_from_id([x for x in range(M)])
    ff0_E = r.get_energies_from_id(dft0_id)
    train_FF_energies -= ff0_E
    train_energies = train_DFT_energies - train_FF_energies # Delta learning
    train_forces = train_DFT_forces - train_FF_forces
    train_xyz = all_xyz[:M]
    # validation set
    r.change_lot("TPSS")
    val_DFT_energies = r.get_energies_from_id([x for x in range(M,_max)]) # methods only produces "forward" indexing 
    val_DFT_forces = r.get_forces_from_id([x for x in range(M,_max)])
    val_DFT_energies -= dft0_E
    r.change_lot("MOFFF")
    val_FF_energies = r.get_energies_from_id([x for x in range(M,_max)])
    val_FF_forces = r.get_forces_from_id([x for x in range(M,_max)])
    val_forces = val_DFT_forces - val_FF_forces
    val_FF_energies -= ff0_E
    val_energies = val_DFT_energies - val_FF_energies # Delta learning
    val_xyz = all_xyz[M:_max]
    r.close()          
    if convert_to_rics == True:
        __m = molsys.mol.from_file("methanol.mfpx")
        bondtable = jnp.array(__m.get_conn_as_tab())
        conn_table = __m.conn
        angletable = gen_angle_table(conn_table)
        dihedraltable = gen_dihedral_table(conn_table)
        ric_tables = (bondtable, angletable, dihedraltable)
        train_rics = mbatch_RICs(train_xyz, ric_tables)
        val_rics = mbatch_RICs(val_xyz, ric_tables)
        return train_energies, train_rics, val_energies, val_rics, train_forces, val_forces
    else:
        return train_DFT_energies, train_FF_energies, train_xyz, train_FF_forces, train_DFT_forces

if __name__ == "__main__":
    from Multi_Layer_Perceptron import mbatch_force_loss, grad_mbatch_RICs, mbatch_ini_MLP,grad_dMLP_dRIC
    train_DFT_energies, train_FF_energies, train_xyz, train_FF_forces, train_DFT_forces = main()
    __m = molsys.mol.from_file("methanol.mfpx")
    bondtable = jnp.array(__m.get_conn_as_tab())
    conn_table = __m.conn
    angletable = gen_angle_table(conn_table)
    dihedraltable = gen_dihedral_table(conn_table)
    ric_tables = (bondtable, angletable, dihedraltable)
    ric = mbatch_RICs(train_xyz, ric_tables)
    dric_dxyz = grad_mbatch_RICs(train_xyz, ric_tables)

    ric = ric[0]
    print(ric)
    fe_fit = NNPlot(name="fe_fit", params_file="force_E_fit/param_hist_fe_fit.pkl", loss_file="force_E_fit/loss_hist_fe_fit.pkl")
    params = fe_fit.params[1]
    ric = jnp.array([1.14952, 1.16573, 1.06156, 1.45821, 0.95515, 2.00171, 1.7388,  1.92461, 1.97874, 1.91508, 1.89703, 1.80227, 0.0, 2.09, 2.09])
    gradient = grad_dMLP_dRIC(params, ric)
    print(gradient)
    exit()
    e, f = mbatch_ini_MLP(ric, params, dric_dxyz)
    e = e - 0.44
    #x = train_DFT_energies
    #y = train_FF_energies
    x = train_DFT_forces.flatten()
    y = train_FF_forces.flatten()
    plt.scatter(x,y, color = "cyan", s = 4, label = "unaugmented")
    #x = train_DFT_energies
    #y = e + train_FF_energies
    x = train_DFT_forces.flatten()
    y = f.flatten() + train_FF_forces.flatten()
    plt.scatter(x,y, color = "purple", s = 4, label = "augmented")
    plt.plot(x,x, color = "black")
    plt.legend()
    plt.ylabel(r"F(FF)")
    plt.xlabel(r"F(DFT)")
    #plt.show()
    plt.savefig(fname = f"error_line_forces.png", dpi = 300, format = "png", bbox_inches = "tight", transparent = False)
    exit()



    #from Multi_Layer_Perceptron import mbatch_loss as fe_loss
    #from Multi_Layer_Perceptron import mbatch_force_loss, grad_mbatch_RICs

    #fe_fit = NNPlot(name="fe_fit", params_file="force_E_fit/param_hist_fe_fit.pkl", loss_file="force_E_fit/loss_hist_fe_fit.pkl") #best id = 1
    #e,ric,vale,valric, train_f, val_f = fe_fit.load_samples(n_samples = 15000, fname = "trainingsdata/3000K_methanol.hdf5", convert_to_rics=True, permu = False)
    #e,xyz,vale,valxyz, train_f, val_f = fe_fit.load_samples(n_samples = 2000, fname = "trainingsdata/methanol.hdf5", convert_to_rics=False, permu = True    )

    #__m = molsys.mol.from_file("methanol.mfpx")
    #bondtable = jnp.array(__m.get_conn_as_tab())
    #conn_table = __m.conn
    #angletable = gen_angle_table(conn_table)
    #dihedraltable = gen_dihedral_table(conn_table)
    #ric_tables = (bondtable, angletable, dihedraltable)
    #inp_rics = mbatch_RICs(xyz, ric_tables)
    #dric_dxyz = grad_mbatch_RICs(xyz, ric_tables)
    #fe_fit.MLP_error_barplot(inp=valric, ref=vale, _id = 1)
    #print(f"F-E-loss: {fe_loss(inp_rics, fe_fit.params[1], dric_dxyz, train_f, e+ 0.495)}")
    #print(f"E-loss: {mbatch_loss(fe_fit.params[1], inp_rics, e+ 0.495)}")
    #print(f"F-loss: {mbatch_force_loss(inp_rics, fe_fit.params[1], dric_dxyz, train_f)}")

    
    


    #############%%%%%%%%%%%%%%%%%%%%%%%%%###############

    #methanol_efit = NNPlot(name="methanol_efit", params_file="methanol_fit/param_hist_corr_dihedral.pkl", loss_file="methanol_fit/loss_hist_corr_dihedral.pkl") # best param id = 1
    #methanol_efit.NN_loss(low=9000, up=11000)
    #methanol_efit.save_fig(dpi=200, nameadd="min_loss")
    #e,ric,vale,valric, train_f, val_f = methanol_efit.load_samples(n_samples = 2000, fname = "trainingsdata/methanol.hdf5", convert_to_rics=True, permu = False)
    #methanol_efit.MLP_error_barplot_force(inp=valric, ref=val_f, _id = 1)
    #methanol_efit.save_fig(dpi = 300, nameadd="bar_test_force_perm")
    #plt.show()
    #methanol_efit.MLP_error_barplot(inp=ric, ref=e, _id = 1, error=True, area = "none")
    #plt.show()
    #methanol_efit.save_fig(dpi=200, nameadd="bar_test_relerror")#_error_lowlimit")

    #fe_fit = NNPlot(name="energy_force_fit", params_file="force_E_fit/param_hist_ftmp.pkl")#params_file="force_E_fit/param_hist_ftmp.pkl")
    #e,ric,vale,valric = fe_fit.load_samples(n_samples = 15000, fname = "trainingsdata/3000K_methanol.hdf5")
    #fe_fit.MLP_error_barplot(inp=ric, ref=e, _id = 1)
    #plt.show()


    #ow_best = NNPlot(name = "ow_bestfit", layers = None)
    #ow_best.loss_overview(stamname = "func_MLP_test/batching/loss_hist_batchsize_2_60_40_60_1_10000_", low = 90000, up = None)
    #plt.show()

    #acti = NNPlot(name = "sigmoid")
    #acti.actifunc_plot(func = jax.nn.gelu)
    #acti.actifunc_plot(func = jax.nn.sigmoid, low=-5, up=5)
    #acti.actifunc_plot(func = jax.numpy.tanh)
    #acti.save_fig()

    #batch_conv = NNPlot(name="convergance_batch_late", layers=None,
    #                    params_file = None, loss_file = None)
    #batch_conv.loss_convergence("func_MLP_test/batching/loss_hist_batchsize_2_60_40_60_1_", identifier = [10,100,1000,10000],low = 90000, up = None, i=0)
    #batch_conv.save_fig(dpi=200)
    #bigA = NNPlot(name = "bigA", layers=None,
    #              params_file= "param_hist_20_40_too_large_a.pkl",
    #
    #              loss_file= "loss_hist_20_40_too_large_a.pkl")
    #bigA.NN_weights_bias()
    #a = bigA.animate_NN_weights_bias()
    #import matplotlib
    #a.save(filename="movi.gif")
    #bigA.NN_loss(low=10,)
    #print(np.array(bigA.params_f))
    #bigA.save_fig()

    ow = NNPlot(name = "overview_best", layers = None,
                  params_file = None , 
                  loss_file = None, axes = None)
    #ow.loss_overview("func_MLP_test/layer_b2/loss_hist_layer_b2_20_40_s", low=9000)
    #ow.loss_overview("func_MLP_test/layer_c/loss_hist_layer_c_40_40_20_s", low=9000)
    ow.loss_overview("func_MLP_test/batching/loss_hist_batchsize_2_60_40_60_1_100_", low=90000)
    plt.show()
    #ow.save_fig()


    #delta_big = NNPlot(name = "func_delta_big", layers = None,
    #                   params_file = "func_MLP_test/batching/param_hist_batchsize_2_60_40_60_1_100_2.pkl")
    #delta_big.DeltaMLPfunc(fname = "func_MLP_test/batching/param_hist_batchsize_2_60_40_60_1_100_2.pkl", _id = 79979)
    #delta_big.save_fig(dpi=200)
    #plt.show() 

    #rics_ana = NNPlot(name="RICs")
    #e,ric,vale,valric, train_f, val_f = rics_ana.load_samples(n_samples = 2000, fname = "trainingsdata/methanol.hdf5", convert_to_rics=True, permu = False)
    #a = np.array(ric.T)[14]
    #s = a.argsort()
    #print(a[s])
    #exit()
    #for i in range(15):
    #    if i < 5:
    #        xlabel = fr"Bondlength {i} [$\AA$]"
    #    elif i < 12:
    #        xlabel = fr"Bondangle {i} [rad]"
    #    else:
    #        xlabel = fr"Dihedralangle {i} [rad]"
    #    rics_ana.inp_histo(inp=ric, _id = i, xlabel = xlabel)
    #   #plt.show()
    #   plt.savefig(fname = f"lowT_RIC_{i}.png", dpi = 100, format = "png", bbox_inches = "tight", transparent = False)
